{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Background ","metadata":{}},{"cell_type":"markdown","source":"## Task description:\n\nIn this competition, we'll **identify and segment functional tissue units (FTUs)** accross **five** human organs:\n\n* Prostate\n* Spleen\n* Lung\n* Kidney\n* Large Intestine\n\nThe challenge in this competition is to build algorithms that **generalize**:\n* across different **organs** and\n* across different **dataset** differences\n\n=> This is a **semantic segmentation** problem.","metadata":{}},{"cell_type":"markdown","source":"## Data description:\n\nThey only release public *Human Protein Atlas (HPA)* data for the training dataset. However, they will release private *HPA* data and *Human BioMolecular Atlas Program (HuBMAP)* for their public test set. For the private test set, they only use *HuBMAP* data.\n\n**File Information**:\n1. `train|test.csv`:\n* `id` - The image ID.\n* `organ` - The organ that the biopsy sample was taken from.\n* `data_source` - Whether the image was provided by Hubamp or HPA.\n* `img_height` - The height of the image in pixels.\n* `img_width` - The width of the image in pixels.\n* `pixel_size` - The height/width of a single pixel from this image in micrometers. All HPA images have a pixel size of 0.4 ¬µm. For Hubmap imagery the pixel size is 0.5 ¬µm for kidney, 0.2290 ¬µm for large intestine, 0.7562 ¬µm for lung, 0.4945 ¬µm for spleen, and 6.263 ¬µm for prostate.\n* `tissue_thickness` - The thickness of the biopsy sample in micrometers. All HPA images have a thickness of 4 ¬µm. The Hubmap samples have tissue slice thicknesses 10 ¬µm for kidney, 8 ¬µm for large intestine, 4 ¬µm for spleen, 5 ¬µm for lung, and 5 ¬µm for prostate.\n* `rle` - The target column. A run length encoded copy of the annotations. Provided for the training set only.\n* `age` - The patient's age in years. Provided for the training set only.\n* `sex` - The sex of the patient. Provided for the training set only.\n2. `train_annotations`: provided in the format of **points that define the boundaries of the polygon masks of the FTUs**\n3. `train|test_images`: the images:\n* Expect roughly 550 images in the hidden test set.\n* All images used have at least one FTU.\n* All tissue data used in this competition is from healthy donors that pathologists identified as pathologically unremarkable tissue.\n* HPA details:\n    * All HPA images are 3000 x 3000 pixels with a tissue area within the image around 2500 x 2500 pixels.\n    * HPA samples were stained with antibodies visualized with 3,3'-diaminobenzidine (DAB) and counterstained with hematoxylin.\n* HuBMAP details:\n    * The Hubmap images range in size from 4500x4500 down to 160x160 pixels.\n    * HuBMAP images were prepared using Periodic acid-Schiff (PAS)/hematoxylin and eosin (H&E) stains.\n4. `sample_submission.csv`:\n* `id`-the image ID\n* `rle`-a run length encoded mask of the FTUs in the image","metadata":{}},{"cell_type":"markdown","source":"## Evaluation metric:\n\nThis competition is evaluated on the mean [Dice coefficient](https://radiopaedia.org/articles/dice-similarity-coefficient#:~:text=The%20Dice%20similarity%20coefficient%2C%20also,between%20two%20sets%20of%20data.). The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:\n\n$$\\frac{2‚àó|ùëã‚à©ùëå|}{|ùëã|+|ùëå|}$$\n\nwhere \n* X is the predicted set of pixels and Y is the ground truth. \n* The Dice coefficient is defined to be 1 when both X and Y are empty. \n\n**Note**: metric is to judge the performance of the model, whereas loss function is to optimize the model.\n\nIn this case, our metric is the **mean Dice coefficient**, and we can use different loss functions like \n* **Dice Loss**\n* **Jaccard Loss**\n* **BCE Loss**\n* **Lovasz Loss**\n* **Tversky Loss**\n\nto optimize our models","metadata":{}},{"cell_type":"markdown","source":"## Submission file format\n\nTo reduce the submission file size, the metric uses run-length encoding on the pixel values.\n\nInstead of submitting an exhaustive list of indices for our segmentation, we will submit pairs of values that contain a start position and a run length\n\nE.g. '1 3' implies starting at pixel 1 and running a total of 3 pixels (1,2,3).\n\nNote that, at the time of encoding, the mask should be **binary**\n* The masks for all objects in an image are joined into a single large mask\n* The value of 0 should indicate pixels that are not masked\n* The value of 1 will indicate pixels that are masked.","metadata":{}},{"cell_type":"markdown","source":"## Methods:\n\n1. Overview:\n    * **Run-length encoding (RLE)**: a form of lossless data compression.\n    \n    Since we already have the RLE masks, we don't need to use the annotations from `.json` file.\n    * **Given**: images (`.tiff`), masks in RLE (we need to convert RLE to binary mask before feeding to our models)\n    * **Predict**: masks then convert to RLE for submission\n2. Data processing:\n    * Resize\n    * Normalize\n3. Data augmentation:\n4. Baseline model: \n    * UNET\n\n5. Testing model:\n    * UNET + pretrained model from previous competition","metadata":{}},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"import os \nimport glob\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.auto import tqdm\n\nimport plotly\nfrom plotly import tools\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.offline as pyo\nimport plotly.io as pio\nimport plotly.graph_objects as go\n#pio.templates.default = 'plotly_white'\nsns.set_theme(style=\"darkgrid\")\nimport cv2\nimport tifffile as tiff\n\nimport warnings\nwarnings.simplefilter(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:55:42.302123Z","iopub.execute_input":"2022-06-25T18:55:42.302527Z","iopub.status.idle":"2022-06-25T18:55:42.310778Z","shell.execute_reply.started":"2022-06-25T18:55:42.302496Z","shell.execute_reply":"2022-06-25T18:55:42.309636Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"## Global Config","metadata":{}},{"cell_type":"code","source":"class config:\n    BASE_PATH = Path(\"../input/hubmap-organ-segmentation/\")\n    TRAIN_CSV_PATH = BASE_PATH / \"train.csv\"\n    TRAIN_IMAGES_PATH = BASE_PATH / \"train_images/\"\n    TRAIN_ANNOTATIONS_PATH = BASE_PATH / \"train_annotations/\" # Not needed","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:17:34.344122Z","iopub.execute_input":"2022-06-25T18:17:34.345290Z","iopub.status.idle":"2022-06-25T18:17:34.351672Z","shell.execute_reply.started":"2022-06-25T18:17:34.345230Z","shell.execute_reply":"2022-06-25T18:17:34.350383Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# Load datasets","metadata":{}},{"cell_type":"markdown","source":"## Train dataset","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(config.TRAIN_CSV_PATH)\ndata.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-06-25T18:17:37.908106Z","iopub.execute_input":"2022-06-25T18:17:37.909041Z","iopub.status.idle":"2022-06-25T18:17:38.089209Z","shell.execute_reply.started":"2022-06-25T18:17:37.908989Z","shell.execute_reply":"2022-06-25T18:17:38.088107Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"## Train images\n\nLoad a single image from `train_images`","metadata":{}},{"cell_type":"code","source":"id_ = 10274\nimg = tiff.imread(str(config.TRAIN_IMAGES_PATH/ f\"{id_}.tiff\"))\nprint(img.shape)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:17:40.772252Z","iopub.execute_input":"2022-06-25T18:17:40.772651Z","iopub.status.idle":"2022-06-25T18:17:40.795152Z","shell.execute_reply.started":"2022-06-25T18:17:40.772618Z","shell.execute_reply":"2022-06-25T18:17:40.794049Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Plot image","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.imshow(img)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:17:45.384539Z","iopub.execute_input":"2022-06-25T18:17:45.385404Z","iopub.status.idle":"2022-06-25T18:17:46.820393Z","shell.execute_reply.started":"2022-06-25T18:17:45.385361Z","shell.execute_reply":"2022-06-25T18:17:46.819309Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Train annotations","metadata":{}},{"cell_type":"markdown","source":"**Run-length encoding (RLE)**: a form of lossless data compression","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/paulorzp/rle-functions-run-length-encode-decode\ndef mask2rle(img): # encoder\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n \ndef rle2mask(mask_rle, shape=(1600,256)): # decoder\n    '''\n    mask_rle: run-length as string formated (start length)\n    shape: (width,height) of array to return \n    Returns numpy array, 1 - mask, 0 - background\n\n    '''\n    s = mask_rle.split()\n    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n    starts -= 1\n    ends = starts + lengths\n    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n    for lo, hi in zip(starts, ends):\n        img[lo:hi] = 1\n    return img.reshape(shape).T","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:17:48.150630Z","iopub.execute_input":"2022-06-25T18:17:48.151360Z","iopub.status.idle":"2022-06-25T18:17:48.160049Z","shell.execute_reply.started":"2022-06-25T18:17:48.151325Z","shell.execute_reply":"2022-06-25T18:17:48.159252Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"mask = rle2mask(data[data[\"id\"]==id_][\"rle\"].iloc[-1], (img.shape[1], img.shape[0]))\nmask.shape","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:17:50.824516Z","iopub.execute_input":"2022-06-25T18:17:50.824892Z","iopub.status.idle":"2022-06-25T18:17:50.843661Z","shell.execute_reply.started":"2022-06-25T18:17:50.824864Z","shell.execute_reply":"2022-06-25T18:17:50.842480Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"### Plot mask","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(mask, cmap='PuRd', alpha=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:57:54.141438Z","iopub.execute_input":"2022-06-25T18:57:54.141851Z","iopub.status.idle":"2022-06-25T18:57:55.337919Z","shell.execute_reply.started":"2022-06-25T18:57:54.141818Z","shell.execute_reply":"2022-06-25T18:57:55.336754Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"markdown","source":"### Overlay image and mask","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(10,10))\nplt.imshow(img)\nplt.imshow(mask, cmap='PuRd', alpha=0.5)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:58:02.482460Z","iopub.execute_input":"2022-06-25T18:58:02.482861Z","iopub.status.idle":"2022-06-25T18:58:04.811500Z","shell.execute_reply.started":"2022-06-25T18:58:02.482830Z","shell.execute_reply":"2022-06-25T18:58:04.810451Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"def plot_mask(image, mask, image_id):\n    plt.figure(figsize=(16, 10))\n    \n    plt.subplot(1, 3, 1)\n    plt.imshow(image)\n    plt.title(f\"Image {image_id}\", fontsize=18)\n    \n    plt.subplot(1, 3, 2)\n    plt.imshow(mask, cmap=\"PuRd\", interpolation='none')\n    plt.title(f\"Mask\", fontsize=18)    \n    \n    plt.subplot(1, 3, 3)\n    plt.imshow(image)\n    plt.imshow(mask, cmap=\"PuRd\", alpha=0.5)\n    plt.title(f\"Image {image_id} + mask\", fontsize=18)    \n    \n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:58:14.299956Z","iopub.execute_input":"2022-06-25T18:58:14.300348Z","iopub.status.idle":"2022-06-25T18:58:14.307464Z","shell.execute_reply.started":"2022-06-25T18:58:14.300318Z","shell.execute_reply":"2022-06-25T18:58:14.306635Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"plot_mask(img, mask, id_)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:58:16.433584Z","iopub.execute_input":"2022-06-25T18:58:16.434581Z","iopub.status.idle":"2022-06-25T18:58:19.932357Z","shell.execute_reply.started":"2022-06-25T18:58:16.434538Z","shell.execute_reply":"2022-06-25T18:58:19.931446Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"# Exploratory Data Analysis","metadata":{}},{"cell_type":"markdown","source":"## Statistical Description","metadata":{}},{"cell_type":"code","source":"def EDA(df):\n    \n    print('\\033[1m' +'EXPLORATORY DATA ANALYSIS :'+ '\\033[0m\\n')\n    print('\\033[1m' + 'Shape of the data (rows, columns):' + '\\033[0m')\n    print(df.shape, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'All columns from the dataframe :' + '\\033[0m')\n    print(df.columns, \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Datatpes and Missing values:' + '\\033[0m')\n    print(df.info(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    for col in df.columns:\n        print('\\033[1m' + 'Unique values in {} :'.format(col) + '\\033[0m',len(data[col].unique()))\n    print('\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Summary statistics for the data :' + '\\033[0m')\n    print(df.describe(include='all'), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n        \n    print('\\033[1m' + 'Memory used by the data :' + '\\033[0m')\n    print(df.memory_usage(), \n          '\\n------------------------------------------------------------------------------------\\n')\n    \n    print('\\033[1m' + 'Number of duplicate values :' + '\\033[0m')\n    print(df.duplicated().sum())\n          \nEDA(data)","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:18:04.082792Z","iopub.execute_input":"2022-06-25T18:18:04.083919Z","iopub.status.idle":"2022-06-25T18:18:04.214232Z","shell.execute_reply.started":"2022-06-25T18:18:04.083879Z","shell.execute_reply":"2022-06-25T18:18:04.213124Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Data visualization","metadata":{}},{"cell_type":"markdown","source":"### Univariate visualization of categorical variables (sex, organ)\n","metadata":{}},{"cell_type":"code","source":"# https://www.kaggle.com/code/toomuchsauce/mental-health-plotly-interactive-viz\ncolumns = ['organ','sex']\ndf = data[columns]\n\nbuttons = []\ni = 0\nvis = [False] * 4\n\nfor col in df.columns:\n    vis[i] = True\n    buttons.append({'label' : col,\n             'method' : 'update',\n             'args'   : [{'visible' : vis},\n             {'title'  : col}] })\n    i+=1\n    vis = [False] * 4\n\nfig = go.Figure()\n\nfor col in df.columns:\n    fig.add_trace(go.Pie(\n             values = df[col].value_counts(),\n             labels = df[col].value_counts().index,\n             title = dict(text = 'Distribution of {}'.format(col),\n                          font = dict(size=18, family = 'monospace'),\n                          ),\n             hole = 0.5,\n             hoverinfo='label+percent',))\n\nfig.update_traces(hoverinfo='label+percent',\n                  textinfo='label+percent',\n                  textfont_size=12,\n                  opacity = 0.8,\n                  showlegend = False,\n                  marker = dict(colors = sns.color_palette('PuRd').as_hex(),\n                              line=dict(color='#000000', width=1)))\n              \n\nfig.update_layout(margin=dict(t=0, b=0, l=0, r=0),\n                  updatemenus = [dict(\n                        type = 'dropdown',\n                        x = 1.15,\n                        y = 0.85,\n                        showactive = True,\n                        active = 0,\n                        buttons = buttons)],\n                 annotations=[\n                             dict(text = \"<b>Choose<br>Column<b> : \",\n                             showarrow=False,\n                             x = 1.06, y = 0.92, yref = \"paper\", align = \"left\")])\n\nfor i in range(1,2):\n    fig.data[i].visible = False\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T18:18:08.316159Z","iopub.execute_input":"2022-06-25T18:18:08.316595Z","iopub.status.idle":"2022-06-25T18:18:08.378133Z","shell.execute_reply.started":"2022-06-25T18:18:08.316562Z","shell.execute_reply":"2022-06-25T18:18:08.377224Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Univariate visualization of non-categorical variables (age)\n","metadata":{}},{"cell_type":"code","source":"fig = make_subplots(rows = 1, cols=1)\n\nfig.append_trace(go.Histogram(\n                        x = data['age'],\n                        nbinsx = 15,\n                        #text = ['16', '500', '562', '149', '26', '5', '1'],\n                        marker =  dict(color=\"#EEC8F9\")),\n                        row=1, col=1)\n\nfig.update_xaxes(        \n        title = dict(text = 'Age',\n                     font = dict(size = 15,\n                                 family = 'monospace')),\n        row=1, col=1,\n        tickfont = dict(size=15, family = 'monospace', color = 'black'),\n        tickmode = 'array',\n        #ticktext = ['20-24','25-29', '30-34','35-39', '40-44','45-49', '50-54','55-59', '60-64','65-69', '70-79','80-99'],\n        ticklen = 8,\n        showline = True,\n        showgrid = True,ticks = 'outside')\n\nfig.update_yaxes(\n        row=1, col=1,\n        title = dict(text = 'Count',\n                     font = dict(size = 15,\n                                 family = 'monospace')),\n    \n        tickfont = dict(size=15, family = 'monospace'),\n        tickmode = 'array',\n        showline = False,\n        showgrid = True)\n\nfig.update_traces(\n                  marker_line_color='black',\n                  marker_line_width = 2,\n                  opacity = 0.6,\n                  row = 1, col = 1)\n\n\nfig.update_layout(height=900, width=1200,\n                  title = dict(text = 'Univariate visualization of non-categorical variables<br> Age Distribution',\n                               x = 0.5,\n                               font = dict(size = 16, color ='#9211B5',\n                               family = 'monospace')),\n                  #plot_bgcolor='#EEC8F9 ',\n                  #paper_bgcolor = '#EEC8F9 ',\n                  showlegend = False)\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-06-25T19:04:14.642253Z","iopub.execute_input":"2022-06-25T19:04:14.642611Z","iopub.status.idle":"2022-06-25T19:04:14.689319Z","shell.execute_reply.started":"2022-06-25T19:04:14.642582Z","shell.execute_reply":"2022-06-25T19:04:14.688462Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}